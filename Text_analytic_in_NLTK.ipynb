{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text analytic in NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNv85AF76EIbcVUU6H/ue0E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaifAlmaliki/NLTK_python/blob/main/Text_analytic_in_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DigQiGi9SCcg"
      },
      "source": [
        "**Install NLTK library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ2fy20LRdpk"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install striprtf\n",
        "!pip install wordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5h_sC4ASN5R"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_MC-VH6d_7i"
      },
      "source": [
        "# Library to Tokenize by punkt\n",
        "nltk.download('punkt')  \n",
        "\n",
        "# Library to Tokenize by wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Library to Tokenize by stopword\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Library to Tokenize by POS TAG\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eI28BNOibfl"
      },
      "source": [
        "**Read RTF files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33EBZkgLV1jb"
      },
      "source": [
        "with open('Abby\\'s Story.rtf', 'r') as file:\n",
        "    file1 = file.read()\n",
        "    \n",
        "with open('Alex\\'s Halloween Story.rtf', 'r') as file:\n",
        "    file2 = file.read()\n",
        "\n",
        "with open('Ella\\'s Story.rtf', 'r') as file:\n",
        "    file3 = file.read()\n",
        "\n",
        "with open('Samuel\\'s Halloween Draft.rtf', 'r') as file:\n",
        "    file4 = file.read()"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWH1JVGhhyhP"
      },
      "source": [
        "**Convert Loaded rtf file to text using strprtf library**\n",
        "\n",
        "It is a simple library to convert rtf files to python strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAd9WLVtfG-N"
      },
      "source": [
        "from striprtf.striprtf import rtf_to_text \n",
        "\n",
        "text1 = rtf_to_text(story1)\n",
        "text2 = rtf_to_text(story2) \n",
        "text3 = rtf_to_text(story3) \n",
        "text4 = rtf_to_text(story4)\n",
        "\n",
        "# convert the text to lowercase\n",
        "text1 = text1.lower()\n",
        "text2 = text2.lower()\n",
        "text3 = text3.lower()\n",
        "text4 = text4.lower()\n",
        "\n",
        "print(text1, \"\\n\\n\")\n",
        "print(text2, \"\\n\\n\")\n",
        "print(text3, \"\\n\\n\")\n",
        "print(text4, \"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1Yi5RspDLMu"
      },
      "source": [
        "**1. Count number of paragraphs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEKN8H20DLh_"
      },
      "source": [
        "def count_paragraph(text):\n",
        "    paragraphcount = 0\n",
        "    linecount = 0\n",
        "\n",
        "    for line in text:\n",
        "        if line in ('\\n', '\\r\\n'):\n",
        "            if linecount == 0:\n",
        "                paragraphcount = paragraphcount + 1\n",
        "            linecount = linecount + 1\n",
        "        else:\n",
        "            linecount = 0\n",
        "    return paragraphcount"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc8Epk5sD90K",
        "outputId": "226b1d56-be09-402b-c3a2-d6368862b3cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"The number of paragraphs in text1:\", count_paragraph(text1))\n",
        "print(\"The number of paragraphs in text2:\", count_paragraph(text2))\n",
        "print(\"The number of paragraphs in text3:\", count_paragraph(text3))\n",
        "print(\"The number of paragraphs in text4:\", count_paragraph(text4))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of paragraphs in text1: 2\n",
            "The number of paragraphs in text2: 39\n",
            "The number of paragraphs in text3: 2\n",
            "The number of paragraphs in text4: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9g4tUaNmajF"
      },
      "source": [
        "**Remove stopword from the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbCIc52imZH5",
        "outputId": "a92f96cb-4fcb-42fa-fd8f-9746d09f13bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Add ! , \" , ? to our stopword list \n",
        "stopwords.append(',')\n",
        "stopwords.append('?')\n",
        "stopwords.append('.')\n",
        "stopwords.append('!')\n",
        "stopwords.append('\"')\n",
        "\n",
        "words1 = word_tokenize(text1)\n",
        "words2 = word_tokenize(text2)\n",
        "words3 = word_tokenize(text3)\n",
        "words4 = word_tokenize(text4)\n",
        "\n",
        "wordsFiltered1 = []\n",
        "wordsFiltered2 = []\n",
        "wordsFiltered3 = []\n",
        "wordsFiltered4 = []\n",
        "\n",
        "for w in words1:\n",
        "  if w not in stopwords:\n",
        "    wordsFiltered1.append(w)\n",
        "\n",
        "for w in words2:\n",
        "  if w not in stopwords:\n",
        "    wordsFiltered2.append(w)\n",
        "\n",
        "for w in words3:\n",
        "  if w not in stopwords:\n",
        "    wordsFiltered3.append(w)\n",
        "\n",
        "for w in words4:\n",
        "  if w not in stopwords:\n",
        "    wordsFiltered4.append(w)\n",
        "\n",
        "print(\"wordsFiltered1: \", wordsFiltered1)\n",
        "print(\"wordsFiltered2: \", wordsFiltered2)\n",
        "print(\"wordsFiltered3: \", wordsFiltered3)\n",
        "print(\"wordsFiltered4: \", wordsFiltered4)\n"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wordsFiltered1:  ['creepy', 'noise', 'creek', 'creek', '“', 'huh', '”', 'said', 'sat', 'bed.i', 'heard', 'noise', 'coming', 'bed', 'grabbed', 'flashlight', 'quickly', 'looked', 'bed', 'creek', 'creek', 'scared', '“', 'um', 'anything', '”', 'said', 'slowly', 'lifted', 'comforter', 'took', 'peak', 'saw', '“', '”', 'exclaimed', '“', 'nothing', '”', 'saw', 'big', 'giant', 'spider', '“', 'ahhhh', '”', 'screamed', 'ran', 'bedroom', 'ran', 'kitchen', 'get', 'paper', 'towel', 'got', 'trudged', 'back', 'stairs', 'got', 'flashlight', 'looked', 'back', 'saw', 'spider', 'realized', 'paper', 'towel', 'way', 'small', 'ran', 'closet', 'grab', 'shoe', 'got', 'shoe', 'went', 'kill', 'spider', 'took', 'shoe', 'stamped', '“', 'whew', '”', 'said', 'close', '\\x00']\n",
            "wordsFiltered2:  ['’', 'school', 'principal', 'would', 'home', 'watching', 'teen', 'titans', '’', 'wedgied', 'matt', 'kzaro', 'last', 'week', '’', 'anything', 'chasing', 'around', 'school', 'one', 'day', 'earlier', ':', '“', '’', 'dan', '”', 'yelled', 'school', 'bully', 'mitchell', 'pikars', '“', 'dan', 'mack', 'kllbrpflgh', '--', 'whatever', '”', 'real', 'name', 'donald', 'friends', 'call', 'dan', '’', 'tell', 'mitchell', 'watches', 'donald', 'duck', 'darted', 'class', 'two', 'minutes', 'homeroom', '“', 'wait', '”', 'called', 'simon', 'best', 'friend', 'ya', 'know', 'two', '“', 'dan', '”', 'called', 'back', '“', 'good', 'see', '”', 'sorry', 'arrived', 'nick', 'time', '“', '’', 'fifteen', 'seconds', 'late', '”', 'mr.', 'mike', 'said', 'unison', '“', 'means', 'detention', '”', 'mr.', 'mike', 'never', 'gives', 'detentions', 'five', 'hours', 'later', ':', 'grumbled', 'slumped', 'toward', 'detention', 'room', 'usually', 'never', 'end', 'thought', 'walked', 'room', 'gave', 'ms.', 'mccault', 'detention', 'slip', '“', '”', 'asked', 'busted', '“', '”', '“', 'nothing', '”', 'grumbled', 'sat', 'chair', 'would', 'miss', 'new', 'episode', 'teen', 'titans', 'stuck', 'detention', '“', '”', 'asked', 'stood', '“', '’', '’', 'job', '’', 'um', 'let', '’', 'see', '”', 'voice', 'trailed', '“', 'go', '”', '“', 'yes', 'may', \"''\", 'darted', 'home', '...', 'teen', 'titans', 'defeated', 'trigon', 'villain', 'well', 'know', 'glenn', 'younger', 'brother', 'watching', 'teen', 'titans', 'missed', 'whole', 'episode', 'two', 'hours', 'later', ':', 'went', 'bed', 'unhappy', 'thought', 'mr.', 'mike', 'dreamed', 'dream', 'stuck', 'room', 'mr.', 'mike', 'looked', 'school', '--', '--', '--', '--', 'sweaty', '“', 'pancakes', '”', 'said', 'giant', 'mean', 'giant', 'weight', 'flattened', 'like', 'pizza', 'pancake', 'matter', 'fact', '“', 'aggh', '”', 'screamed', 'three', 'hours', 'later', ':', 'groaned', 'toward', 'gym', 'mr.', 'mike', 'gave', 'detention', '...', 'gym', 'ms.', 'millicent', '-', 'millie', 'short', 'looked', 'like', 'mr.', 'mike', 'weird', '“', 'helloooooooo', '”', 'waved', 'hand', 'front', 'face', 'fast', 'cobra', 'grabbed', 'arm', 'threw', 'door', 'one', 'hour', 'later', ':', 'someone', 'tapped', 'shoulder', '“', 'office', 'school', '”', 'whispered', 'voice', 'spun', 'around', 'principal', 'mr.', 'keens', 'twenty-five', 'minutes', 'later', ':', 'stuck', 'principal', '’', 'office', '“', 'donald', '”', 'asked', 'mr.', 'keens', '’', 'tell', 'anybody', 'real', 'name', 'neither', 'parents', '(', 'cuz', '’', 'want', 'duh', ')', 'wait', '’', 'imagining', 'strange', 'lights', 'outside', 'bedroom', 'window', 'real', 'bolted', 'asked', '“', '”', '“', 'yes', 'ennarg', 'terrible', 'planet', 'kzontos', \"''\", '“', '’', 'cha', 'mean', 'ennard', '”', '“', 'ennarg', '”', '“', 'okay', 'egg-head', '”', 'laughed', 'darted', 'door', 'could', 'peeled', 'face', 'jardled', 'hallway', 'threw', 'mrs.', 'clarr', '’', 'classroom', 'grabbed', 'metal', 'ruler', 'door', 'supposedly', 'said', '“', 'braiiiins', '”', '(', 'mr.', 'keens', ')', 'hurdled', 'ruler', 'catching', 'square', 'nose', 'howled', 'pain', 'took', 'baseball', 'bat', 'gym', '...', 'fell', 'face', 'turns', '’', 'fall', 'face', 'mr.', 'keens`', 'zombie', 'teacher', \"'s\", 'army', 'fell', 'causing', 'fall', 'face', 'sorta', 'like', 'd.b', 'cooper', 'hijacking', 'zombies', '’', 'hijacking', 'read', 'online', 'article', 'hey', 'maybe', 'flight', 'mh370', 'malaysian', 'airlines', 'disappeared', 'zombies', '’', 'assuming', 'beijing', 'malaysia', 'nowhere', 'near', 'bermuda', 'triangle', 'smacked', 'aside', 'zombies', 'one', 'shot', 'admit', 'pretty', 'impressive.then', 'barreled', 'door', 'zombie', 'keens', '“', 'dude', 'ya', 'nearly', 'broke', 'bones', 'ooooh', 'could', 'died', '”', 'said', 'weird', '“', '’', 'already', 'dead', '’', 'zombie', '”', 'reply', 'sorta', 'sounds', 'like', 'cyborg', 'teen', 'titans', 'cyborg', '’', 'part', 'teen', 'titans', 'song', 'says', 'goes', 'something', 'like', 'think', ':', 'cyborg', 'said', 'booyah', 'booyah', 'mr.', 'cannonblaster', 'cyborg', 'wooh', 'mr.', 'hightech', 'master', 'rest', 'titans', 'would', 'say', ':', 'whawhawhawhawhatwhat', 'cyborg', ':', 'mr.', 'meatballdisaster', 'titans', ':', 'whawhawhawhawhatwhat', 'cyborg', ':', 'mr', 'bonbonblaster', 'see', 'daydreaming', 'cyborg', 'keens', 'flipped', 'toward', 'ducked', 'sending', 'flying', 'wall', '“', 'curse', 'mr.', 'donald', 'mack', '”', 'screamed', '“', 'watch', 'phineas', 'ferb', '”', '“', '”', 'asks', 'ceiling', 'collapsed', 'knocked', 'woke', 'start', 'glenn', 'tv', '“', 'brains', '”', 'said', 'froze', 'end', '\\x00']\n",
            "wordsFiltered3:  ['dolloween', 'halloween', 'night', 'johnny', 'getting', 'ready', 'halloween', 'party', 'super', 'excited', 'johnny', 'head', '“', 'ding', 'dong.', '”', 'doorbell', 'party', '’', 'start', 'hour', 'half', 'wondered', 'could', 'opened', 'door', '’', 'see', 'anything', 'looked', 'saw', 'doll', 'old', 'beat', 'looked', 'like', '1000', 'years', 'johnny', 'picked', 'smelled', 'like', 'rotten', 'seaweed', 'disgusting', 'suddenly', 'doll', 'began', 'speak', 'said', 'something', 'like', '“', 'get', 'last', 'thing', 'do.', '”', 'johnny', 'scared', 'threw', 'door', 'hour', 'half', 'later', 'party', 'started', 'great', 'party', 'going', 'smoothly', 'party', 'ended', 'started', 'get', 'ready', 'bed', 'heard', 'something', 'smash', 'ran', 'briskly', 'saw', 'doll', 'ground', 'laughing', 'strange', 'bolted', 'bed', 'shoved', 'pillow', 'head', 'hard', 'time', 'sleeping', 'waited', 'till', 'would', 'fall', 'asleep', 'next', 'day', 'forgotten', 'doll', '\\x00']\n",
            "wordsFiltered4:  ['store', 'found', 'costume', 'put', 'saw', 'blue', 'hedgehog.it', 'saw', 'started', 'run', 'started', 'run', 'ran', 'faster', 'looked', 'realized', 'knuckles', 'knuckles', 'red', 'green', 'white', 'character', 'sonic', 'dash', 'recognized', 'friend', 'sonic', 'chasing', 'sonic', 'character', 'sonic', 'dash.i', 'say', 'hello', 'play', 'friend', 'sonic.we', 'run', 'reach', 'forest.s', 'sonic', 'says', 'forest', 'go', 'forest.the', 'forest', 'lots', 'trees', 'play', 'lot', 'play', 'leaves', 'run', 'chase', 'also', 'play', 'characters', 'sonic', 'dash', 'time', 'go', 'home.i', 'say', 'bye', 'bye.i', 'go', 'home', 'get', 'home', 'parents', 'say', 'you.i', 'say', 'samuel', 'take', 'costume.then', 'parents', 'recognize', 'me.finally', 'say', 'hungry.my', 'parents', 'say', 'dinner', 'say', 'yay', 'dinner', 'together.then', 'say', 'tired.my', 'parents', 'say', 'ok', 'go', 'sleep', 'end', '\\x00']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXySrDRK3say"
      },
      "source": [
        "**2. Tokenize and Count the Number of Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "How9xjvQeJ9C"
      },
      "source": [
        "# File 1\n",
        "from nltk.tokenize import sent_tokenize\n",
        "print(\"The Number of Sentences in file1 is:\", len(sent_tokenize(text1)), \"\\n\")\n",
        "sent_tokenize(text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3ck0HmI30Rv"
      },
      "source": [
        "# File 2\n",
        "from nltk.tokenize import sent_tokenize\n",
        "print(\"The Number of Sentences in file1 is:\", len(sent_tokenize(text2)), \"\\n\")\n",
        "sent_tokenize(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7A9dwZ_d4-y"
      },
      "source": [
        "# File 3\n",
        "from nltk.tokenize import sent_tokenize\n",
        "print(\"The Number of Sentences in file1 is:\", len(sent_tokenize(text3)), \"\\n\")\n",
        "sent_tokenize(text3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i44fyHPl32hL"
      },
      "source": [
        "# File 4\n",
        "from nltk.tokenize import sent_tokenize\n",
        "print(\"The Number of Sentences in file1 is:\", len(sent_tokenize(text4)), \"\\n\")\n",
        "sent_tokenize(text4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZLrzgFEomqs"
      },
      "source": [
        "**4. Classify nouns, pronouns, adjectives and adverbs. (Speech Tagging)**\n",
        "\n",
        "**5. Count number of distinct adjectives and adverbs.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC6HqEo5-0tY"
      },
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from collections import Counter"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNztw3ZDomLe"
      },
      "source": [
        "# file 1\n",
        "sentences = nltk.sent_tokenize(text1)\n",
        "for s in sentences:\n",
        "  \n",
        "  # Classify nouns, pronouns, adjectives and adverbs.\n",
        "  tagged = nltk.pos_tag(nltk.word_tokenize(s))\n",
        "  print(tagged )\n",
        "\n",
        "  # Count number of distinct adjectives and adverbs.*\n",
        "  counts = Counter(tag for word, tag in tagged)\n",
        "  print (\"The Distinct count of adjectives and adverbs are:\", counts, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3VlbriK4ija"
      },
      "source": [
        "# file 2\n",
        "sentences = nltk.sent_tokenize(text2)\n",
        "for s in sentences:\n",
        "  \n",
        "  # Classify nouns, pronouns, adjectives and adverbs.\n",
        "  tagged = nltk.pos_tag(nltk.word_tokenize(s))\n",
        "  print(tagged )\n",
        "\n",
        "  # Count number of distinct adjectives and adverbs.*\n",
        "  counts = Counter(tag for word, tag in tagged)\n",
        "  print (\"The Distinct count of adjectives and adverbs are:\", counts, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAGOiuyy4itP"
      },
      "source": [
        "# file 3\n",
        "sentences = nltk.sent_tokenize(text3)\n",
        "for s in sentences:\n",
        "  \n",
        "  # Classify nouns, pronouns, adjectives and adverbs.\n",
        "  tagged = nltk.pos_tag(nltk.word_tokenize(s))\n",
        "  print(tagged )\n",
        "\n",
        "  # Count number of distinct adjectives and adverbs.*\n",
        "  counts = Counter(tag for word, tag in tagged)\n",
        "  print (\"The Distinct count of adjectives and adverbs are:\", counts, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_p6yrN84i1q"
      },
      "source": [
        "# file\n",
        "sentences = nltk.sent_tokenize(text4)\n",
        "for s in sentences:\n",
        "  \n",
        "  # Classify nouns, pronouns, adjectives and adverbs.\n",
        "  tagged = nltk.pos_tag(nltk.word_tokenize(s))\n",
        "  print(tagged )\n",
        "\n",
        "  # Count number of distinct adjectives and adverbs.*\n",
        "  counts = Counter(tag for word, tag in tagged)\n",
        "  print (\"The Distinct count of adjectives and adverbs are:\", counts, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}